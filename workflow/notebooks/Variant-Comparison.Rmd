---
title: "Replicate Comparison"
author: "Will Hannon"
date: "9/8/2020"
output: html_document
---


```{r Setup, include=FALSE}
require("knitr")
knitr::opts_chunk$set(echo = FALSE)

## ==== Important file paths ==== ##
samples.filepath =  "../../config/samples.csv" # Samples that ran analysis
multiqc.filepath = "../../config/data/2020-08-07_multiQC-data.csv" # QC data (manually prepared from MultiQC)
coverage.filepath = "../../results/split/merged.bedgraph" # Path to coverage data
variant.filepath = "../../results/variant/variants.csv" # Combined Variant Calls
```

```{r Required Packages, message=FALSE, warning=FALSE, echo=FALSE}

## ==== Install Required Packages ==== ##

## List of all packages needed -- non-BioManager installed
packages = c("tidyverse", "kableExtra", "gridExtra", "grid", "UpSetR", "ggridges", "ggpubr", "scales", "gplots")
## Check that packages are installed, if not, install them
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}
## Packages loading
invisible(lapply(c(packages), library, character.only = TRUE))

```

### Overview 

This notebook aims to assess our ability to call genomic variants accurately by comparing replicate sequencing runs. 

The samples consist of 24 sequencing runs with a Ct value of less than 20. Originally, samples were sequenced once as part of [this study](https://doi.org/10.1101/2020.08.13.20173161). The runs with a low enough Ct value to be considered for re-sequencing by a shotgun metagenomic method were re-sequenced as replicates. One sample is currently excluded from this analysis because it is not included on the SRA (`SpID: 10110`), leaving 23 final samples.

By comparing the allele frequency of variants called in both replicates, we hope to assess the accuracy, the optimal frequency cutoff, and the optimal coverage cutoff for downstream bottleneck analyses. 




### Variant Calling 

For this preliminary analysis, I aligned the samples with `BWA,` and called variants with `Varscan`. Ideally, all variant alleles will be identified by counting the number of bases over each position in the genome. However, to quickly get an idea of samples' quality, I decided to approximate this with `Varscan`. I removed any filters for minimum allele frequency, depth, or supporting reads.

```{r Data Import, echo=F, warning=F, message=F}

## =========== Import the variant calls ========== ##
# Filtering out everything but SNPs from Varscan/BWA
variant.df = read_csv(variant.filepath)  %>%
      mutate(Type = case_when(nchar(REF) < nchar(ALT) ~ "Insertion",
                              nchar(REF) > nchar(ALT) ~ "Deletion",
                              nchar(REF) == nchar(ALT) ~ "SNP")) %>% 
      filter(Aligner == "BWA", Caller == "varscan", Type == "SNP") %>% 
      separate(col = Accession, into = c("Accession", "Replicate")) %>% 
      mutate(Replicate = ifelse(is.na(Replicate), 1, Replicate),
             SNP = paste0(REF, POS, ALT))

# Reshape the variant data to combine replicates
combined.df = inner_join(filter(variant.df, Replicate == 1),
                        filter(variant.df, Replicate == 2),
                        by = c( "Accession", "POS", "REF", "ALT"),
                        suffix = c(".one", ".two")) %>% 
              select(!c("Replicate.one", "Replicate.two", "Aligner.one", "Aligner.two"))

## =========== Import the run quality information ========== ##
multiqc.df = read.csv(multiqc.filepath) %>% 
  separate(col = Sample, into = c("Accession", "Aligner"), sep = "\\.") %>% 
  separate(col = Accession, into = c("Accession", "Replicate"), sep = "-") %>% 
  mutate(Replicate = ifelse(is.na(Replicate), 1, Replicate)) %>% 
  mutate(Depth =(reads_mapped*average_length) / 29903) %>% 
  filter(Aligner == "BWA") 


## =========== Coverage ========== ##
coverage.df = read.delim(coverage.filepath) %>% 
  mutate(POS = (Stop-Start)/2 + Start, BinSize = Stop-Start) %>% 
  separate(col = Accession, into = c("Accession", "Replicate"), sep = "-")  %>% 
  mutate(Replicate = ifelse(is.na(Replicate), 1, Replicate)) %>% 
  filter(POS < 29850) # Filtering out the last 53 bases becuase of Poly-A

```


### Replicate Information

The critical difference between the replicates is the processing. The samples on the SRA underwent a slightly different pre-processing pipeline to remove human reads and trim adaptors. Eventually, I'll re-process the raw files to make the pipelines identical. 

Here, I'm comparing the difference in the number of mapped reads between replicates. There are some substantial differences between replicates, but I'm not sure if this is because of processing or sequencing differences. 


```{r Mapped Reads, echo=F, message=F, warning=F, fig.width=15, fig.height=7, fig.align='center'}

## ==== Plot the mapped reads per sample ==== ##

reads.mapped.plt = multiqc.df %>% 
  ggplot(aes(x = reorder(Accession, -reads_mapped), y = reads_mapped, fill = Replicate)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    scale_fill_manual(values = c("#3271a8", "#bf6021")) + 
    scale_y_continuous(label=scientific_format()) +
    xlab("Accession") +
    ylab("Reads Mapped") +
    ggtitle("Number of Mapped Reads per Sample" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=13,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1))

  
## ==== Plot the % of mapped reads per sample ==== ##

prec.reads.mapped.plt =  multiqc.df %>% 
  ggplot(aes(x = reorder(Accession, -reads_mapped_percent), y = reads_mapped_percent, fill = Replicate)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    scale_fill_manual(values = c("#3271a8", "#bf6021")) + 
    xlab("Accession") +
    ylab("Reads Mapped (%)") +
    ggtitle("Percent of Mapped Reads per Sample" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=13,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1))

## ==== Join the plots together ==== ##
grid.arrange(reads.mapped.plt, prec.reads.mapped.plt, 
             layout_matrix = rbind(c(1, 1, 2, 2),
                                   c(1, 1, 2, 2)))
```

### Coverage 

I calculated the coverage over the genome in 50bp increments. I flitered out the last 53 bp (2 bins) of the genome becuase there is a polyA tract with a tremendous amount of reads that align. These skew the coverage visualization. Figuring out a better way to align this section is important.

```{r Coverage, echo=F, message=F, warning=F, fig.width=15, fig.height=7, fig.align='center'}
coverage.df %>% 
  ggplot(aes(x = POS, y = Depth, col = Replicate)) + 
    geom_line() + 
    facet_wrap(~Accession) +
    scale_y_continuous(labels=label_scientific()) +
    scale_fill_manual(values = c("#3271a8", "#bf6021")) + 
    xlab("Position") +
    ylab("Depth of Coverage") +
    ggtitle("Mean Depth of Coverage over the Genome" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=13,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) 

```


### Consensus Variants

The optimal way to classify and filter consensus variants is to do it before reads are aligned, and the final variant alleles are called. However, by identifying consensus variants at this stage, I can check for any systematic biases or samples that might skew the cohort's consensus sequence. 

To identify consensus variants, I filtered for any variants that appear at greater than or equal to 50% frequency in either replicate and counted the number of samples with these variants. 

```{r Consensus Variants, echo=F, warning=F, message=F, fig.align='center'}

## ==== Determine the variants present in every sample ==== ##
consensus.df =combined.df %>% 
  filter(AF.one >= 0.5 | AF.two >= 0.5 ) %>% 
  select(SNP.one, Accession) %>% 
  group_by(SNP.one) %>% 
  summarize(Count = n()) %>% 
  arrange(-Count) %>% 
  mutate(Consesus = case_when(Count == 23 ~ "Yes",
                              Count >= 21 ~ "Almost",
                              Count < 21 ~ "No")) 

## ==== Visually represent this ==== ##
consensus.df %>% 
  ggplot(aes(x = reorder(SNP.one, -Count), y = Count, fill = Consesus)) + 
    geom_bar(stat = "identity", position = position_dodge()) +
    scale_fill_manual(values = c("#dfe615", "#757575", "#b51714")) + 
    xlab("SNP") +
    ylab("Number of Samples") +
    ggtitle("Consensus Variants" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=13,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1)) + 
    theme(legend.position = "none")
    

```

There are two SNPs (indicated in Red) that show up in all 23 samples. There are another ~7 SNPs that are likely consensus variants, but are missing from either one or two samples (or replicates). 

I was curious if these SNPs present in 21 or 22 samples were genuinely absent from the remaining samples. I checked if there was a bias in what samples were missing consensus variants. 

```{r Missing Consensus, echo=F, warning=F, message=F}

## ==== Is there a systematic bais in the samples here? ==== ##

consensus.snps = filter(consensus.df, Count >= 21 & Count < 23 )$SNP.one

for (i in 1:length(consensus.snps)){
  # Get the SNP
  snp = consensus.snps[i]
  
  # == Determine the sample or samples missing this SNP == #
  
  # All accessions
  accessions = unique(combined.df$Accession)
  
  # Accessions that have this SNP
  snp.accessions = unique(filter(combined.df, SNP.one == snp)$Accession)
  
  # Which accessions aren't included?
  print(paste(snp, accessions[!accessions %in% snp.accessions], sep = "-"))

}
```

There are two samples (`SRR11939959` and `SRR11939986`) that are missing three and four variants respectivley. 

```{r Missing Consensus Vallidation, echo=F, warning=F, message=F}
## ==== Check if these samples are only missing variants in one of the replicates ==== ##

missing.sampels = c("SRR11939959", "SRR11939986", "SRR11939958")

multiqc.df[which(multiqc.df$Accession %in% missing.sampels),] %>%
  select(Accession, Replicate, `Reads Mapped` = "reads_mapped", `Percent Mapped` = "reads_mapped_percent", Depth) %>% 
  kable(caption = "Samples Missing Consensus Variants") %>%
  kable_styling(bootstrap_options = c("striped")) 

```

Samples `SRR11939959` and `SRR11939986` have at least one replicate with a very low percentage of reads mapped. The following SNPs were excluded becuase they were missing from a single replicate: 

- `A23403G`
- `C14408T`
- `C241T`
- `G25563T`
- `G11083T`

These SNPs do appear to be true consensus SNPs. Two SNPs are genuinely missing from both replicates of `SRR11939986` and can't be easily explained by low coverage:

- `C7564T`
- `G28376T`

Therefore, the following 7 SNPs are the true consensus SNPs and should be masked from downstream analyses. 
```{r True Consensus SNPs, echo=F, warning=F, message=F}

## ==== Manually select the true consensus SNPs ==== ##

true.consensus = filter(consensus.df, Count >= 21)[which(!filter(consensus.df, Count >= 21)$SNP.one %in% c("C7564T", "G28376T")),]$SNP.one

print(true.consensus)

# Mask the consensus SNPs
combined.df = combined.df[which(!combined.df$SNP.one %in% true.consensus),]
variant.df = variant.df[which(!variant.df$SNP %in% true.consensus),]

```

### Replicate Comparison

Next, I wanted to assess the variability in the frequency of alternative alleles called in each replicate. Samples with significant variability likely have low viral template number and can be excluded from downstream analyses. Additionally, I should be able to determine which samples need to be sequenced to higher depth. 

I removed any samples with fewer than 3 alternative alleles at sites with at least 200X coverage. The result were 14 final samples, listed below.

```{r Remove Samples, echo=F, warning=F, message=F}

## ==== Filter samples with too few variants ==== ##

include.accession = combined.df %>% 
  filter(DP.one >= 200 & DP.two >= 200) %>% 
  group_by(Accession) %>% 
  count() %>% 
  filter(n >= 3) %>% 
  pull(Accession)

combined.include.df = combined.df[which(combined.df$Accession %in% include.accession),]

print(include.accession)
```


```{r Variant Comparison, echo=F, warning=F, message=F, fig.width=10, fig.height=10, fig.align='center'}

## ==== Plot alternative allele frequencies against each other ==== ##

# Raw variant frequencies with line of best fit and correlation coeff.
combined.include.df %>% 
  ggplot(aes(x = (AF.one), y = (AF.two))) + 
    facet_wrap(~Accession) +
    geom_smooth(method='lm', col = "red", se = F) + 
    stat_cor(method="pearson") +
    geom_point() +
    xlab("Allele Frequency Replicate One") +
    ylab("Allele Frequency Replicate Two") +
    ggtitle("Variation Between Replicate SNPs" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1)) 

```

Generally, the correlation between variant allele frequencies looks high in all samples. However, this could be skewed by the samples' heavily bimodal distribution, with high-frequency variants driving the correlation. Not many variants exist at intermediate frequencies. The variability in allele frequencies is better visualized after a log10 transformation. 

```{r Variant Comparison Log10, echo=F, warning=F, message=F, fig.width=10, fig.height=10, fig.align='center'}

## ==== Plot alternative allele frequencies against each other ==== ##

# Raw variant frequencies with line of best fit and correlation coeff.
combined.include.df %>% 
  ggplot(aes(x = log10(AF.one), y = log10(AF.two))) + 
    facet_wrap(~Accession) +
    geom_smooth(method='lm', col = "red", se = F) + 
    stat_cor(method="pearson") +
    geom_point() +
    xlab("Allele Frequency Replicate One (log10)") +
    ylab("Allele Frequency Replicate Two (log10)") +
    ggtitle("Variation Between Replicate SNPs" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1))

```

Even though these variants were processed with no filters, it looks like there are fewer than the variants called by varscan with filters. This is because the last round included InDels, which I have excluded from this analysis. It might make sense to see what these correlations look like with InDels included.

```{r Variant Comparison with InDels, echo=F, warning=F, message=F, fig.width=10, fig.height=10, fig.align='center'}

## =========== Import the variant calls, keep indels ========== ##
# Keep only Varscan/BWA
variant.df.indels = read_csv(variant.filepath)  %>%
      mutate(Type = case_when(nchar(REF) < nchar(ALT) ~ "Insertion",
                              nchar(REF) > nchar(ALT) ~ "Deletion",
                              nchar(REF) == nchar(ALT) ~ "SNP")) %>% 
      filter(Aligner == "BWA", Caller == "varscan") %>% 
      separate(col = Accession, into = c("Accession", "Replicate")) %>% 
      mutate(Replicate = ifelse(is.na(Replicate), 1, Replicate),
             SNP = paste0(REF, POS, ALT))

# Reshape the variant data to combine replicates
combined.df.indels = inner_join(filter(variant.df.indels, Replicate == 1),
                        filter(variant.df.indels, Replicate == 2),
                        by = c( "Accession", "POS", "REF", "ALT"),
                        suffix = c(".one", ".two")) %>% 
              select(!c("Replicate.one", "Replicate.two", "Aligner.one", "Aligner.two"))


## ==== Mask consensus variants ==== ##

# Mask the consensus SNPs
combined.df.indels = combined.df.indels[which(!combined.df.indels$SNP.one %in% true.consensus),]

## ==== Filter samples with too few variants ==== ##

include.accession.indels = combined.df.indels %>% 
  filter(DP.one >= 200 & DP.two >= 200) %>% 
  group_by(Accession) %>% 
  count() %>% 
  filter(n >= 3) %>% 
  pull(Accession)

combined.include.df.indels = combined.df.indels[which(combined.df.indels$Accession %in% include.accession.indels),]


## ==== Plot alternative allele frequencies against each other ==== ##

# Raw variant frequencies with line of best fit and correlation coeff.
combined.include.df.indels %>% 
  ggplot(aes(x = log10(AF.one), y = log10(AF.two))) + 
    facet_wrap(~Accession) +
    geom_smooth(method='lm', col = "red", se = F) + 
    stat_cor(method="pearson") +
    geom_point() +
    xlab("Allele Frequency Replicate One (log10)") +
    ylab("Allele Frequency Replicate Two (log10)") +
    ggtitle("Variation Between Replicate SNPs & InDels" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1)) 

```

### Correlates of Low-Quality

Finally, I wanted to see which factors correlated the most strongly with low-quality replicates. The two factors I investigated were the Ct of the sample and the mean depth of the two replicates. 

```{r Limiting Factor, echo=F, warning=F, message=F, fig.width=15, fig.height=7, fig.align='center'}

diff.df = combined.include.df %>% 
  select(AF.one, AF.two, Accession, Avg.Ct = "avg_ct.one") %>% 
  mutate(AF.diff = abs(AF.one - AF.two)) %>% 
  group_by(Accession, Avg.Ct) %>% 
  summarize(Avg.AF.diff = mean(AF.diff)) 

multiqc.avg.df = multiqc.df %>% group_by(Accession) %>% summarize(Mean_Depth = mean(Depth), Mean_Mapped = mean(reads_mapped))

limiting.df = full_join(diff.df, multiqc.avg.df, by = "Accession")

## ==== Comparison between Ct and replicates ==== ##
ct.plt = limiting.df %>% 
  ggplot(aes(x = Avg.Ct, y= Avg.AF.diff)) + 
    geom_point(size = 3) + 
    geom_smooth(se=F, col = "red") + 
    ggtitle("Variation v. Ct" ) +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) 

## ==== Comparison between mapped reads and replicates ==== ##
depth.plt = limiting.df %>% 
  ggplot(aes(x = Mean_Depth, y = Avg.AF.diff)) + 
    geom_point(size = 3) + 
    geom_smooth(se=F, col = "red") + 
    ggtitle("Variation v. Mean Depth") +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18,  family="mono")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) 

## ==== plot both together ==== ##

grid.arrange(ct.plt, depth.plt, 
             layout_matrix = rbind(c(1, 1, 2, 2),
                                   c(1, 1, 2, 2)))

```

It seems like Ct value doesn't correlate particularly well with the average difference between variant allele frequencies. There is a more clear relationship between mean depth and the average difference in allele frequencies. 

### Shared Variants

```{r Pairwise, echo=F, warning=F, message=F, fig.width=7, fig.height=7, fig.align='center'}

combined.include.df.indels.minor = filter(combined.include.df.indels, AF.one < 0.5 | AF.two < 0.5)

accessions.to.include = unique(combined.include.df.indels$Accession)
results = data.frame()
for (i in 1:length(accessions.to.include)){
  for (j in 1:length(accessions.to.include)){
    accession.i = accessions.to.include[i]
    accession.j = accessions.to.include[j]
    variants.i = filter(combined.include.df.indels.minor, Accession == accession.i)$SNP.one
    variants.j = filter(combined.include.df.indels.minor, Accession == accession.j)$SNP.one
    sum.variants = length(variants.i) + length(variants.j)
    intersect = intersect(variants.i, variants.j)
    n_intersect = length(intersect)
    row = data.frame(accession.i, accession.j, n_intersect)
    results = rbind(results, row)
  }
}


df  = spread(results, accession.i, n_intersect)[,-1]
rownames(df) = spread(results, accession.i, n_intersect)[,1]
df = as.matrix(df)

heatmap.2(df, scale = "none", col = viridis_pal(),
          trace = "none", density.info = "none", cexRow=1,cexCol=1,margins=c(12,8),srtCol=45)

```

For the most part, the runs seem to cluster based on whether they have many variants or few variants. This is probably due to the depth. Should we make the case for sequncing to higher depth?


```{r}
df
```


















