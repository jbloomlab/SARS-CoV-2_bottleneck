---
title: "2. Quality Control"
author: "Will Hannon"
date: "1/11/2021"
output: html_document
---

The goal of this notebook is check some of the non-coverage related aspects of library quality. I also look at some of the background data, including Ct values for each sample, the number of people who were infected on the boat, and more. This notebook provides background and context for the outbreak samples.

```{r Setup, include=FALSE}
require("knitr")
knitr::opts_chunk$set(echo = FALSE)
```

```{r Required Packages, message=FALSE, echo=FALSE}

## ==== Install Required Packages ==== ##

## List of all packages needed -- non-BioManager installed
packages = c("tidyverse", "scales")
## Check that packages are installed, if not, install them
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}
## Packages loading
invisible(lapply(c(packages), library, character.only = TRUE))

```

```{r Paths to data, echo = FALSE}

# Path to a processed data frame from the tool Multiqc.
# This isn't exhaustive for all QC data collated by this tool. 
# For a quick look at individual samples, there is also an HTML
# repot generated in the `results/qc` directory.
multiqc.data = "../../results/qc/formatted_multiqc_data.csv"

# Path to the data for what % of each library are filted out by BBduk
filter.data = "../../results/qc/BBduk_filtered_reads.csv"

# Path to sample information
sample.data = "../../config/samples.csv"

# Path to boat metadata
boat.data = "../../config/data/Boat_Sample_Metadata.csv"

```


```{r Setup}

# Colors for plotting
experiment.colors = c("#000000", "#7a7a7a")

```


```{r Load in data, warning=FALSE, message = FALSE}

# QC data from multiqc
multiqc.df = read_csv(multiqc.data)
# QC data from BBduk, collated separatley
filter.df = read_csv(filter.data) %>% mutate(library = as.character(library))
# Join the BBduk data with the multiqc data
qc.df = left_join(multiqc.df, filter.df, by = c("sample", "replicate", "library"))


# Sample and boat information
sample.df = read_csv(sample.data) %>% select(LibraryLayout, Virus, Host, Source, SpID ="spID") %>% distinct()

boat.df = read_csv(boat.data) 

metadata.df = full_join(boat.df, sample.df, by = "SpID") %>% 
  mutate(Included = if_else(is.na(Source), "Not Included", "Included"))

```

## Context for the Samples

This donut plot shows a breakdown of all the passangers on the boat by the status of their avaliable data. There were a total of 122 people (113 men and 9 women) on the ship. The ship returned on *Day 18* after a crew member became symptomatic and required hospitalization. 

According to the original paper (in section **"Testing after ship returned due to outbreak."**), RT-PCR testing data was avaliable for all 122 passengers upon 
return to shore - however, only 118 individuals had RT-QPCR testing from the week of return. 

Of those 122 passengers, 101 tested positive within the first week + 10 days of return (positive = Ct < 35). The remaining three seroconverted and were considered infected on the ship. Of those 104 infections, 39 consensus genomes were assembled (Ct < 26). 

```{r Donut, echo = F, message = F, warning = F, fig.width=6, fig.height=6, fig.align='center'}

donut.df = data.frame(Category = c("Negative", "Postive + Low Quality", "Positve + Deep-Sequenced"),
                      Count = c(18, 65, 39)) %>% 
  mutate(Freq = Count/sum(Count))

# Compute the cumulative percentages (top of each rectangle)
donut.df$ymax <- cumsum(donut.df$Freq)

# Compute the bottom of each rectangle
donut.df$ymin <- c(0, head(donut.df$ymax, n=-1))

# Compute label position
donut.df$labelPosition <- (donut.df$ymax + donut.df$ymin) / 2

ggplot(donut.df, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=Category)) +
  geom_rect() +
  geom_label(x=3.5, aes(y=labelPosition, label=Count), size=8, fill = "white") +
  scale_fill_brewer(palette=9) +
  coord_polar(theta="y") +
  xlim(c(2, 4)) +
  theme_void(35) +
  theme(legend.position = "none")

ggsave("../../results/figures/donut_plot_summary.svg", device = svg(), width = 5, height = 5)

```

## Ct Values of Samples

The Ct values are shown for 72 samples collected from infected passangers of the boat that were **avaliable for resequencing**. Of those 72 samples, 24 met the stringent *20 Ct or lower* cutoff for viral RNA levels. These samples were selected for our analysis to be re-sequenced with Illumina NextSeq.

```{r Samples included by Ct, message = F, warning = F, fig.width=11, fig.height=5, fig.align='center'}

metadata.df %>% 
  ggplot(aes(x = as.factor(reorder(SpID, -avg_ct)), y = avg_ct, col = Included)) + 
    geom_hline(yintercept = 20, col = "black", linetype = 2, size = 1.5) + 
    geom_point(size = 4) +
    annotate("text", x = 55, y = 25, label = paste0("N = ", length(filter(metadata.df, avg_ct <= 20)$SpID)), size = 11) + 
    xlab("Sample") + 
    ylab("Cycle Threshold (Ct)") + 
    scale_color_manual(values = c("#005AB5","#DC3220")) + 
    theme_classic(30) +
    theme(axis.text.x=element_blank(), 
          axis.ticks.x=element_blank(),
          legend.position = "none") 

ggsave("../../results/figures/included_samples_by_Ct.svg", device = svg(), width = 11, height = 5)

```

## Library Size

Here, I looked at the size of each sequencing run. This is pretty similar to coverage, but most studies report this information in the paper as well. This includels reads that are 'PCR Duplicates', although that distinction doesn't make much sense when dealing with small genomes at high depth. It's likely that real reads will be codified as dulicates by chance in these circumstances. 

This figure shows the number of reads comprising each individual library. There is a significant amount of variability between the library size of different replicates and samples. The median number of mapped reads was 1,113,690 per library/replicate. For whatever reason, the first replciate of 10027 has an insane number of reads. 

```{r Run Size, message = FALSE, warning=FALSE, fig.width=11, fig.height=5, fig.align='center'}

# The number of mapped reads for each sample. (1) correspond to the initial sequencing. (2) corresponds to the supplementary sequencing.
# `merged` corresponds to the combindation of the two libraries. 

median_lib_size = median(filter(qc.df, library == "merged")$reads_mapped)

qc.df %>% 
  filter(library == "merged") %>% 
  ggplot(aes(x = reorder(as.factor(sample), -reads_mapped), y = reads_mapped, fill = as.factor(replicate))) +
    geom_bar(stat = "identity", position = position_dodge()) + 
    geom_hline(yintercept = median_lib_size, col = "red") + 
    scale_fill_manual(values = experiment.colors, name = "Replicate") +
    xlab("Sample") +
    ylab("Number of Mapped Reads") +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18)) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1))

print(paste("The median library size is", median_lib_size, "reads."))

ggsave("../../results/figures/library_size_by_replicate.svg", device = svg(), width = 11, height = 5)

```

## Percent Filtered

Here, I wanted to look at the percentage of reads that map to SARS-CoV-2. Because this is metagenomic sequencing (a.k.a. no amplification), there will be reads that do not map to SARS-CoV-2. It's possible that the percentage of SARS-CoV-2 specific reads might be correlated with the underlying template number, and thus our ability to accuratley assess variants between the replicates. 

Here is the percent of the sample that deemed to belong to SARS-CoV-2 as opposed to some other organism. The percentage of reads that was filtered is pretty all over the place, with some samples nearly 100% SARS-CoV-2 and others closer to 5% SARS-CoV-2. There is also significant amount of difference between replicates. This highlights the importance of looking at replicates. 

```{r Percent Filtered replicates, message = FALSE, warning=FALSE, fig.width=11, fig.height=5, fig.align='center'}

qc.df %>%
  filter(library != "merged") %>% 
  group_by(sample, replicate) %>% 
  summarize(mean_percent_viral_reads = mean(percent_filtered_viral_reads)) %>% 
  ggplot(aes(x = as.factor(sample), y = mean_percent_viral_reads, fill = as.factor(replicate))) +
    geom_bar(stat = "identity", position = position_dodge()) + 
    scale_fill_manual(values = experiment.colors, name = "Replicate") +
    scale_y_continuous(labels = percent) + 
    xlab("Individual") +
    ylab("Percent SARS-CoV-2 Reads") +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18)) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(axis.text.x=element_text(angle=45, hjust=1))

ggsave("../../results/figures/precent_sars-cov-2_reads.svg", device = svg(), width = 11, height = 5)

```

