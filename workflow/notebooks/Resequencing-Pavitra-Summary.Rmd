---
title: "Resequencing-Pavitra"
author: "Will Hannon"
date: "10/8/2020"
output: powerpoint_presentation
---

```{r Setup, include=FALSE}
require("knitr")
knitr::opts_chunk$set(echo = FALSE)
```

```{r Required Packages, message=FALSE, warning=FALSE, echo=FALSE}

## ==== Install Required Packages ==== ##

## List of all packages needed -- non-BioManager installed
packages = c("tidyverse", "ggrepel", "plotly")
## Check that packages are installed, if not, install them
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}
## Packages loading
invisible(lapply(c(packages), library, character.only = TRUE))

```

```{r Filepaths, include=FALSE}
## ==== Important file paths ==== ##

percent.filtered.data = "../../config/data/filtered-reads.csv" 
reads.mapped.data = "../../results/qc/multiqc/multiqc_report_data/multiqc_samtools_stats.txt" 
sample.data = "../../config/samples.csv" 
pileup.data = "../../results/pileup/raw-variants.csv" 
depth.data = "../../results/coverage/merged.depth"
percent.depth.data = "../../results/coverage/merged.average.depth"
coverage.data = "../../results/coverage/coverage.stats"

```

```{r Process Data, echo=F, warning=F, message=F}

## ==== Import data ==== ##

# The percentage of each run that's SARS-CoV-2
percent.filtered.df = read_csv(percent.filtered.data) %>%
  separate(col = `Sample Name`, sep = "\\.", remove = T, into = c("sample", "action")) %>% 
  mutate(`Percent filtered` = as.numeric(gsub("%", "", `Percent filtered`))) %>% 
  select(sample, percent = "Percent filtered")

# The number of reads mapping to SARS-CoV-2 from `samtools stats`
reads.mapped.df = read_tsv(reads.mapped.data) %>%
  separate(col = Sample, sep = "\\.", remove = T, into = c("sample", "action", "isolate")) %>% 
  filter(action == "BWA", isolate == "virus") %>% 
  select(!c("action", "isolate"))

# Information about the samples from Pavitra including Ct
sample.data.df = read_csv(sample.data)  %>%
  filter(Percent == "100") %>% 
  select(sample = "Run", replicate = "Replicate", accession = "SpID", Ct = "avg_ct", gisaid_accession, ON_TGT)

# The percentage of the genome covered by more than 200 bases 
percent.adequate.depth.df = read.table(percent.depth.data, header = T) %>% 
  rename(sample = "Accession", percent_200X = "Percent")

# Info from samtools coverage
samtools.coverage.df = read.table(coverage.data, header = T) %>% 
  separate(filename, into = c("X1", "X2", "X3", "sample", "X4"), sep = "/") %>% 
  select(!starts_with("X"))

# The depth of bases meetinq quality filter in 50 bp bins. 
depth.bins.df = read.table(depth.data, header = T)

## ==== Join the data frames that giver per replicate info==== ##

metadata.df = merge(percent.filtered.df, reads.mapped.df) %>% 
              merge(., sample.data.df) %>% 
              merge(., percent.adequate.depth.df) %>% 
              merge(., samtools.coverage.df)


## ==== Summarize the number of SNPs called ==== ##
pileup.snp.df = read_csv(pileup.data)

# Reshape the variant data to combine replicates
combined.pileup.snp.df = full_join(filter(pileup.snp.df, Replicate == 1),
                               filter(pileup.snp.df, Replicate == 2),
                               by = c("Accession", "POS", "REF", "ALT", "CONS", "Strand"),
                               suffix = c(".one", ".two")) %>% 
  select(!c("Replicate.one", "Replicate.two")) %>% 
  filter(Strand == "+/-", AF.one > 0 & AF.two > 0) %>% 
  rename(DP.one = "Depth.one", DP.two = "Depth.two") %>% 
  mutate_each(funs(replace(., which(is.na(.)), 0))) %>% 
  rename(accession = "Accession")

```

```{r Sample Quality, message=F, warning=F, echo=F}

## ===== Set the sample quality based on concordance ===== ##
good.samples = c("10128", "10131", "10130", "10040", "10102", "10129", "10042")
medium.samples = c("10088", "10089", "10094", "10117", "10118", "10127", "10029")
poor.samples = c("10160", "10107", "10114", "10136", "10138", "10027", "10028", "10091", "10110", "10106", "10039")


metadata.df = metadata.df %>% 
  mutate(Quality = case_when(accession %in% good.samples ~ "good",
                             accession %in% medium.samples ~ "medium",
                             accession %in% poor.samples ~ "poor"))  


# Order the factors and assign colors
metadata.df$Quality = factor(metadata.df$Quality, levels = c("good", "medium", "poor"))

# Colors
quality.colors = c("#00d640", "#d6d200", "#d60000")

combined.pileup.snp.df = combined.pileup.snp.df  %>% 
  mutate(Quality = case_when(accession %in% good.samples ~ "good",
                             accession %in% medium.samples ~ "medium",
                             accession %in% poor.samples ~ "poor"))  

combined.pileup.snp.df$Quality = factor(combined.pileup.snp.df$Quality, levels = c("good", "medium", "poor"))

```

####

Here is a quick summary of the concordance between replicates and which samples might need to be sequenced again. 

####

Here are all of the alleles identified in one replicate plotted against all alleles identified in the second replicate by frequency. In this plot, I’m not considering any minimum allele frequency or depth. The only filtering criteria is a minimum Base Quality Phred score of 25 (~ 0.3% chance of erroneous base call). 

####

```{r Replicate Comparison, echo=F, warning=F, message=F, fig.width=15, fig.height=15, fig.align='center'}

combined.pileup.snp.df %>% 
  ggplot(aes(x = (AF.one), y = (AF.two))) + 
  facet_wrap(~accession, ncol = 4) +
  geom_point(size = 5, col = "black", alpha = 0.5) +
  geom_smooth(method='lm', se = F, col = "black") + 
  xlab("Allele Frequency Replicate One") +
  ylab("Allele Frequency Replicate Two") +
  ggtitle("Replicate Allele Frequency Concordance") + 
  theme_classic() +
  theme(legend.position="bottom", legend.box = "horizontal") +
  theme(legend.box.background = element_rect(colour = "black")) +
  theme(text=element_text(size=18,  family="Helvetica")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
  theme(panel.grid.major.x = element_line(colour="grey", linetype="dashed")) +
  theme(panel.background = element_rect(fill = NA, color = "black"))

```

####

```{r Replicate Comparison Log10, echo=F, warning=F, message=F, fig.width=15, fig.height=15, fig.align='center'}

combined.pileup.snp.df %>% 
  ggplot(aes(x = log10(AF.one), y = log10(AF.two))) + 
  facet_wrap(~accession, ncol = 4) +
  geom_point(size = 5, col = "black", alpha = 0.5) +
  geom_smooth(method='lm', se = F, col = "black") + 
  xlab("Allele Frequency Replicate One (Log10)") +
  ylab("Allele Frequency Replicate Two (Log10)") +
  ggtitle("Replicate Allele Frequency Concordance (Log10)")  +
  theme_classic() +
  theme(legend.position="bottom", legend.box = "horizontal") +
  theme(legend.box.background = element_rect(colour = "black")) +
  theme(text=element_text(size=18,  family="Helvetica")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
  theme(panel.grid.major.x = element_line(colour="grey", linetype="dashed")) +
  theme(panel.background = element_rect(fill = NA, color = "black"))

```

####

Here is the same plot, but this time I’ve identified all of the alleles by color if they were present at sites covered by more than 200 reads and present at an allele frequency greater then 0.5%. Ideally, we’d like to identify alleles at this frequency and greater, making 200 base observations at each site the bare minimum. 

####

```{r Replicate Comparison Color, echo=F, warning=F, message=F, fig.width=15, fig.height=15, fig.align='center'}

AF = 0.005
DP = 200

combined.pileup.snp.df %>% 
    ggplot(aes(x = log10(AF.one), y = log10(AF.two))) + 
    facet_wrap(~accession, ncol = 4) +
    geom_point(size = 3, alpha = 0.5, col = "#9c9c9c") +
    geom_point(data = filter(combined.pileup.snp.df, DP.one >= DP & DP.two >= DP, AF.one >= AF & AF.two >= AF), 
               mapping = aes(x = log10(AF.one), y = log10(AF.two)), size = 3, col = "#a30000") +
    geom_smooth(data = filter(combined.pileup.snp.df, DP.one >= DP & DP.two >= DP, AF.one >= AF & AF.two >= AF), 
                mapping = aes(x = log10(AF.one), y = log10(AF.two)),
                method='lm', se = F, col = "#a30000") +
    geom_hline(yintercept = log10(AF), size = 0.5, linetype = 2) +
    geom_vline(xintercept = log10(AF), size = 0.5, linetype = 2) +
    xlab("Allele Frequency Replicate One (Log10)") +
    ylab("Allele Frequency Replicate Two (Log10)") +  
    ggtitle("Replicate Allele Frequency Concordance (AF >= .5%, DP >= 200)")  +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18,  family="Helvetica")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(panel.background = element_rect(fill = NA, color = "black"))
        

```

####

Using these alleles to identify concordance, I qualitatively lumped samples into categories based on whether they had good concordance, medium concordance, or bad concordance. 

####

```{r Replicate Comparison Catagories, echo=F, warning=F, message=F, fig.width=15, fig.height=15, fig.align='center'}

combined.pileup.snp.df %>% 
    ggplot(aes(x = log10(AF.one), y = log10(AF.two), col = Quality)) + 
    facet_wrap(~accession, ncol = 4) +
    geom_point(size = 3, alpha = 0.5) +
    geom_point(data = filter(combined.pileup.snp.df, DP.one >= DP & DP.two >= DP, AF.one >= AF & AF.two >= AF), 
               mapping = aes(x = log10(AF.one), y = log10(AF.two)), size = 3, col = "black") +
    geom_smooth(data = filter(combined.pileup.snp.df, DP.one >= DP & DP.two >= DP, AF.one >= AF & AF.two >= AF), 
                mapping = aes(x = log10(AF.one), y = log10(AF.two)),
                method='lm', se = F, col = "black") +
    geom_hline(yintercept = log10(AF), size = 0.5, linetype = 2) +
    geom_vline(xintercept = log10(AF), size = 0.5, linetype = 2) +
    scale_color_manual(values = quality.colors) +
    xlab("Allele Frequency Replicate One (Log10)") +
    ylab("Allele Frequency Replicate Two (Log10)") +
    ggtitle("Replicate Allele Frequency Concordance (AF >= .5%, DP >= 200)")  +
    theme_classic() +
    theme(legend.position="bottom", legend.box = "horizontal") +
    theme(legend.box.background = element_rect(colour = "black")) +
    theme(text=element_text(size=18,  family="Helvetica")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) +
    theme(panel.background = element_rect(fill = NA, color = "black"))
        
```


####

Next, I wanted to see if concordance for each sample was determined by depth or template number and whether re-sequencing could remedy the observed discrepancies. 

```{r Replicate Disparity, echo=F, warning=F, message=F, fig.width=10, fig.height=10, fig.align='center'}

metadata.df %>% 
  ggplot(aes(y = percent, x = percent_200X, col = Quality)) + 
  geom_point(size = 3) +
  geom_text_repel(aes(label = accession),
                size = 3, box.padding = 0.4)  +
  ylab("Precent SARS-CoV-2") +
  xlab("Percent of Genome with 200X Depth") +
  ggtitle("Relationship between % SARS and % Coverage") +
  scale_color_manual(values = quality.colors) +
  theme_classic() +
  theme(legend.position="bottom", legend.box = "horizontal") +
  theme(legend.box.background = element_rect(colour = "black")) +
  theme(text=element_text(size=13,  family="Helvetica")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) 


```

####

As you can see, some replicates have very little of their genome coverage by more than 200 base observations at each site. These samples likely need to be dropped or sequenced again. 

####

```{r Replicate Disparity 2, echo=F, warning=F, message=F, fig.width=10, fig.height=10, fig.align='center'}

metadata.df %>% 
  ggplot(aes(y = percent, x = numreads, col = Quality)) + 
  geom_point(size = 3) +
  geom_text_repel(aes(label = accession),
                size = 3, box.padding = 0.4)  +
  ylab("Precent SARS-CoV-2") +
  xlab("Numbver of Mapped Reads") +
  ggtitle("Relationship between % SARS and Mapped Reads") +
  scale_color_manual(values = quality.colors) +
  theme_classic() +
  theme(legend.position="bottom", legend.box = "horizontal") +
  theme(legend.box.background = element_rect(colour = "black")) +
  theme(text=element_text(size=13,  family="Helvetica")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.grid.major.y = element_line(colour="grey", linetype="dashed")) 

```

####

Here, I’ve plotted each replicate on the coordinates of the percent of SARS-CoV-2 reads in the initial library by the average coverage over the genome. It’s clear than some samples have huge differences between the coverage. For those with reasonably high % but few reads, we recommend sequencing from cDNA. 

####

In this way, I can determine which samples need to be re-sequenced by looking at the average features of the samples with good concordance. 

For those samples with high Ct and low % mapping in both replicates, we don’t recommend sequencing again. 

For those samples with one replicate with a very low % of SARS-CoV-2 and the other with adequate sequencing, we should re-prep another sample from scratch. 

 
```{r Output CSV, include=FALSE}

write_csv(metadata.df, "../../config/data/2020-10-09_SeqData.csv")

```


### How much to resequence? 

1,000,000 SARS-CoV-2 reads = ((amount to resequence) * fraction SARS-CoV-2)  

(1,000,000 SARS-CoV-2 reads - current # of reads) = (amount to resequence * fraction SARS-CoV-2) 

(1,000,000 SARS-CoV-2 reads - current # of reads)/fraction SARS-CoV-2 = amount to resequence


```{r}

# Get the target number of reads as the mean number of mapped reads for the 
# samples with "good" concordance
target_reads = metadata.df %>% 
  group_by(Quality) %>% 
  summarize(mean_mapped = mean(reads_mapped)) %>% 
  filter(Quality == "good") %>% 
  pull(mean_mapped)

# Calculate the number of reads that still need to be sequenced
resequence.df = metadata.df %>% 
  # Calculate the number of reads using all mapped SARS-CoV-2 reads / % kmer matched from initial library
  mutate(target_mapped_reads = (target_reads - reads_mapped) / (percent/100)) %>% 
  # If we already have enough reads, set the value to 0 or no resequencing
  mutate(target_mapped_reads = if_else(target_mapped_reads < 0, 0, target_mapped_reads)) %>% 
  # Calculate the cost assuming the 10,000,000 reads costs 200$
  mutate(cost = (target_mapped_reads/10000000) * 200) %>% 
  # Select only the relevant columns
  select(sample, accession, replicate, Ct, percent, reads_mapped, average_length, meandepth, percent_200X, Quality, target_mapped_reads, cost) 
  
# Print the total cost of resequencing
print(sum(resequence.df$cost))

# Print the total number of reads to resequence 
print(sum(resequence.df$target_mapped_reads))

# Write out the data table to send to Pavitra
write_csv(resequence.df, "../../config/data/2020-10-20_SeqData-target.csv")

 
```







